{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries and packages\n",
    "import pandas as pandas\n",
    "import numpy as numpy\n",
    "import matplotlib.pyplot\n",
    "from IPython.display import Markdown, display\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data from csv file and Separating table into values(x) and outcome(y)\n",
    "\n",
    "data_table = pandas.read_csv('C:/Users/ademo/Dropbox/Archive/Archive/historical_table_adjusted_2.csv', sep=',', header=0)\n",
    "x = data_table.values[:,0:9]\n",
    "y = data_table.values[:,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into training dataset and testing dataset: train (1457 rows), test (365 rows)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=10, test_size = 0.2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7178082191780822"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying Support vector machine (SVM) Classifier to Data\n",
    "\n",
    "from sklearn import svm\n",
    "svm_classifier = svm.SVC(kernel='linear', gamma='auto', C=2)\n",
    "svm_classifier.fit(x_train, y_train)\n",
    "svm_classifier.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9397260273972603"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying Random Forest Classifier to Data\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=120)\n",
    "rf.fit(x_train, y_train)\n",
    "rf.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>req_priority</th>\n",
       "      <th>tech_rank</th>\n",
       "      <th>opport</th>\n",
       "      <th>support_type</th>\n",
       "      <th>source_req</th>\n",
       "      <th>proj_type</th>\n",
       "      <th>type_feature</th>\n",
       "      <th>bu</th>\n",
       "      <th>numProd_line</th>\n",
       "      <th>total_effort</th>\n",
       "      <th>release</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>38</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>23</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   req_priority  tech_rank  opport  support_type  source_req  proj_type  \\\n",
       "0             2          5       3             1           4         44   \n",
       "1             2         11       3             1          15         25   \n",
       "2             2         12       0             1          15         38   \n",
       "3             1         23       3             1          15         26   \n",
       "4             1         22       0             1          15         23   \n",
       "\n",
       "   type_feature  bu  numProd_line  total_effort  release  \n",
       "0             1   4             1           0.0        0  \n",
       "1             1   4             2           9.0        1  \n",
       "2             5   1             2           5.0        0  \n",
       "3             1   4             1          11.0        1  \n",
       "4             5   2             3           3.0        0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5589041095890411"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying Naive Bayes Multinomial Classifier to Data\n",
    "\n",
    "m_nb = MultinomialNB()\n",
    "m_nb.fit(x_train, y_train)\n",
    "m_nb.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6986301369863014"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying  Logistic Regression Classifier to Data \n",
    "\n",
    "lr = LogisticRegression(solver = 'liblinear', multi_class='auto')\n",
    "lr.fit(x_train, y_train)\n",
    "lr.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7315068493150685"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying Decision Tree Classifier to data\n",
    "    \n",
    "dtc = DecisionTreeClassifier(criterion = \"entropy\", random_state=100, max_depth=3, min_samples_leaf=5)\n",
    "dtc.fit(x_train, y_train)\n",
    "dtc.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9450926561427591"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying K-Nearest Neighbor (KNN) Classifier to data\n",
    "# preprocessing values for KNN\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "sc_x = StandardScaler()\n",
    "x_train = sc_x.fit_transform(x_train)\n",
    "x_test= sc_x.transform(x_test)\n",
    "\n",
    "KNN_classifier = KNeighborsClassifier(n_neighbors=2, p=2, \n",
    "                                      metric='euclidean')\n",
    "KNN_classifier.fit(x_train, y_train)\n",
    "KNN_classifier.score(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing K - Fold on Data\n",
    "\n",
    "K-Folds cross-validator\n",
    "\n",
    "Provides train/test indices to split data in train/test sets. Split\n",
    "dataset into k consecutive folds (without shuffling by default).\n",
    "\n",
    "Each fold is then used once as a validation while the k - 1 remaining\n",
    "folds form the training set. \n",
    "\n",
    "K-fold allows all the data to be used in training and testing of the data in other to allow the model to maximise learning and testing with all available data. This helps the model to learn better and give more accurate classification and prediction.\n",
    "\n",
    "K- fold does a specified round of learning on the Data and in each round it changes the part of the data used in testing and training to maximise the learning of the patterns in the table and maximise prediction result\n",
    "table is split into 20 parts and the correspoing parts are used for trainnig and testing <br>\n",
    "\n",
    "|Round|  Testing|Training|\n",
    "|-----|-------|------|\n",
    "|1    | 5  6  7  8  9 10 11 12 13 14 15 16 17 18 19     |  0 1 2 3 4     |\n",
    "|2    | 0  1  2  3  4 10 11 12 13 14 15 16 17 18 19      |  5 6 7 8 9      |\n",
    "|3    | 0  1  2  3  4  5  6  7  8  9 15 16 17 18 19     | 10 11 12 13 14     |\n",
    "|4    | 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14      | 15 16 17 18 19     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KFold(n_splits=4, random_state=None, shuffle=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# implementing K-fold on dataset\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "k_fold = KFold(n_splits=4)\n",
    "k_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5  6  7  8  9 10 11 12 13 14 15 16 17 18 19] [0 1 2 3 4]\n",
      "[ 0  1  2  3  4 10 11 12 13 14 15 16 17 18 19] [5 6 7 8 9]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 15 16 17 18 19] [10 11 12 13 14]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14] [15 16 17 18 19]\n"
     ]
    }
   ],
   "source": [
    "# creating indices for k-fold\n",
    "\n",
    "for train_index, test_index in k_fold.split([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]):\n",
    "    print(train_index, test_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a function to run supplied classifier/algorithm and return score\n",
    "The function take classifier/model type and run it against the test and train data to get a score for the specific model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function that takes specific model type and data, use data to learn and test to test model and return score\n",
    "\n",
    "def get_score(model, x_train, x_test, y_train, y_test):\n",
    "    model.fit(x_train, y_train)\n",
    "    \n",
    "    return model.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a sparate fucntion for KNN\n",
    "\n",
    "def get_score_KNN(model, x_train, x_test, y_train, y_test):\n",
    "#    sc_x = StandardScaler()\n",
    "    x_train = sc_x.fit_transform(x_train)\n",
    "    x_test = sc_x.transform(x_test)\n",
    "\n",
    "    #KNN_classifier = KNeighborsClassifier()\n",
    "    model.fit(x_train, y_train)\n",
    "#    KNN_classifier.score(x_train, y_train)\n",
    "    return model.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing array to store each models' score from the k-fold iterations\n",
    "scores_svm = []\n",
    "scores_KNN = []\n",
    "scores_rf = []\n",
    "scores_lr = []\n",
    "scores_n_bayes = []\n",
    "scores_dtc = []\n",
    "scores_knnc = []\n",
    "\n",
    "for train_index, test_index in k_fold.split(x):\n",
    "    x_train, x_test, y_train, y_test = x[train_index], x[test_index], y[train_index], y[test_index]\n",
    "    \n",
    "    scores_svm.append(get_score(SVC(), x_train, x_test, y_train, y_test))\n",
    "    scores_KNN.append(get_score_KNN(KNeighborsClassifier(n_neighbors=2, p=2, metric='euclidean'), x_train, x_test, y_train, y_test))\n",
    "    scores_rf.append(get_score(RandomForestClassifier(), x_train, x_test, y_train, y_test))\n",
    "    scores_lr.append(get_score(LogisticRegression(solver = 'liblinear', multi_class='ovr'), x_train, x_test, y_train, y_test))    \n",
    "    scores_n_bayes.append(get_score(MultinomialNB(), x_train, x_test, y_train, y_test))\n",
    "    scores_dtc.append(get_score(DecisionTreeClassifier(), x_train, x_test, y_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8961228968544257"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scores_KNN\n",
    "x_train = sc_x.fit_transform(x_train)\n",
    "x_test = sc_x.transform(x_test)\n",
    "KNN_classifier.score(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7472527472527473"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_score_KNN(KNeighborsClassifier(n_neighbors=2, p=2, metric='euclidean'), x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Result in each fold of training**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM: [0.4342105263157895, 0.5855263157894737, 0.589010989010989, 0.6021978021978022] Avg = 0.5527364083285137\n",
      "\n",
      "NBA: [0.4342105263157895, 0.5855263157894737, 0.6527472527472528, 0.6945054945054945] Avg = 0.5917473973395027\n",
      "\n",
      "LRA: [0.6359649122807017, 0.7039473684210527, 0.6263736263736264, 0.621978021978022] Avg = 0.6470659822633507\n",
      "\n",
      "RFT: [0.8399122807017544, 0.8947368421052632, 0.945054945054945, 0.8] Avg = 0.8699260169654905\n",
      "\n",
      "DTC: [0.8289473684210527, 0.8837719298245614, 0.9362637362637363, 0.7692307692307693] Avg = 0.8545534509350299\n",
      "\n",
      "KNN: [0.7653508771929824, 0.7982456140350878, 0.8813186813186813, 0.7472527472527473] Avg = 0.7980419799498747\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avg_svm = sum(scores_svm)/ len(scores_svm)\n",
    "avg_n_bayes  = sum(scores_n_bayes)/ len(scores_n_bayes)\n",
    "avg_lr = sum(scores_lr)/ len(scores_lr)\n",
    "avg_KNN = sum(scores_KNN)/ len(scores_KNN)\n",
    "avg_r_forest = sum(scores_rf)/ len(scores_rf)\n",
    "avg_dtc = sum(scores_dtc)/ len(scores_dtc)\n",
    "\n",
    "display(Markdown('**Result in each fold of training**'))\n",
    "\n",
    "print ('SVM: {0} Avg = {1}'.format(scores_svm, avg_svm))\n",
    "print(\"\")\n",
    "print ('NBA: {0} Avg = {1}'.format(scores_n_bayes, avg_n_bayes))\n",
    "print(\"\")\n",
    "print ('LRA: {0} Avg = {1}'.format(scores_lr, avg_lr))\n",
    "print(\"\")\n",
    "print ('RFT: {0} Avg = {1}'.format(scores_rf, avg_r_forest))\n",
    "print(\"\")\n",
    "print ('DTC: {0} Avg = {1}'.format(scores_dtc, avg_dtc))\n",
    "print(\"\")\n",
    "print ('KNN: {0} Avg = {1}'.format(scores_KNN, avg_KNN))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing Result of various classifier on Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Support Vector Machine Classifier Result**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.55      1.00      0.71       248\n",
      "         1.0       0.00      0.00      0.00       207\n",
      "\n",
      "    accuracy                           0.55       455\n",
      "   macro avg       0.27      0.50      0.35       455\n",
      "weighted avg       0.30      0.55      0.38       455\n",
      "\n",
      "Accuracy is =  54.505494505494504\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ademo\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Naive Bayes - Multinomial Classifier Result**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.76      0.35      0.48       248\n",
      "         1.0       0.53      0.87      0.66       207\n",
      "\n",
      "    accuracy                           0.59       455\n",
      "   macro avg       0.65      0.61      0.57       455\n",
      "weighted avg       0.66      0.59      0.56       455\n",
      "\n",
      "Accuracy is =  58.681318681318686\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Random Forest Classifier Result**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.05      0.09       248\n",
      "         1.0       0.46      0.99      0.63       207\n",
      "\n",
      "    accuracy                           0.48       455\n",
      "   macro avg       0.66      0.52      0.36       455\n",
      "weighted avg       0.68      0.48      0.34       455\n",
      "\n",
      "Accuracy is =  47.69230769230769\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "***K-Nearest Neighbor Classifier Result***"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.98      0.92       248\n",
      "         1.0       0.98      0.82      0.89       207\n",
      "\n",
      "    accuracy                           0.91       455\n",
      "   macro avg       0.92      0.90      0.91       455\n",
      "weighted avg       0.92      0.91      0.91       455\n",
      "\n",
      "Accuracy is =  90.98901098901099\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Logistic Regression Classifier Result**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.55      1.00      0.71       248\n",
      "         1.0       1.00      0.01      0.02       207\n",
      "\n",
      "    accuracy                           0.55       455\n",
      "   macro avg       0.77      0.50      0.36       455\n",
      "weighted avg       0.75      0.55      0.39       455\n",
      "\n",
      "Accuracy is =  54.94505494505495\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Decision Tree Classifier Result**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.55      1.00      0.71       248\n",
      "         1.0       0.00      0.00      0.00       207\n",
      "\n",
      "    accuracy                           0.55       455\n",
      "   macro avg       0.27      0.50      0.35       455\n",
      "weighted avg       0.30      0.55      0.38       455\n",
      "\n",
      "Accuracy is =  54.505494505494504\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_predict_svm = svm_classifier.predict(x_test)\n",
    "y_predict_KNN = KNN_classifier.predict(x_test)\n",
    "y_predict_rf = rf.predict(x_test)\n",
    "y_predict_m_nb = m_nb.predict(x_test)\n",
    "y_predict_lr = lr.predict(x_test)\n",
    "y_predict_dtc = dtc.predict(x_test)\n",
    "\n",
    "\n",
    "print('')\n",
    "display(Markdown('**Support Vector Machine Classifier Result**'))\n",
    "print(classification_report(y_test, y_predict_svm))\n",
    "print(\"Accuracy is = \", accuracy_score(y_test, y_predict_svm)*100)\n",
    "print('')\n",
    "print('')\n",
    "\n",
    "display(Markdown('**Naive Bayes - Multinomial Classifier Result**'))\n",
    "print(classification_report(y_test, y_predict_m_nb))\n",
    "print(\"Accuracy is = \", accuracy_score(y_test, y_predict_m_nb)*100)\n",
    "print('')\n",
    "print('')\n",
    "\n",
    "display(Markdown('**Random Forest Classifier Result**'))\n",
    "print(classification_report(y_test, y_predict_rf))\n",
    "print(\"Accuracy is = \", accuracy_score(y_test, y_predict_rf)*100)\n",
    "print('')\n",
    "print('')\n",
    "\n",
    "print('')\n",
    "display(Markdown('***K-Nearest Neighbor Classifier Result***'))\n",
    "print(classification_report(y_test, y_predict_KNN))\n",
    "print(\"Accuracy is = \", accuracy_score(y_test, y_predict_KNN)*100)\n",
    "print('')\n",
    "print('')\n",
    "\n",
    "display(Markdown('**Logistic Regression Classifier Result**'))\n",
    "print(classification_report(y_test, y_predict_lr))\n",
    "print(\"Accuracy is = \", accuracy_score(y_test, y_predict_lr)*100)\n",
    "print('')\n",
    "print('')\n",
    "\n",
    "display(Markdown('**Decision Tree Classifier Result**'))\n",
    "print(classification_report(y_test, y_predict_dtc))\n",
    "print(\"Accuracy is = \", accuracy_score(y_test, y_predict_dtc)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1.\n",
      " 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
      " 1. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 0.\n",
      " 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1.\n",
      " 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1.\n",
      " 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0.\n",
      " 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0.\n",
      " 0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print (y_predict_KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 1. ... 1. 0. 0.]\n",
      "0.4862788144895719\n"
     ]
    }
   ],
   "source": [
    "print (sse)\n",
    "print(numpy.mean(sse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4862788144895719\n"
     ]
    }
   ],
   "source": [
    "print (mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1822,)\n",
      "(1822, 9)\n",
      "[1. 0. 1. ... 1. 0. 0.]\n",
      "0.4862788144895719\n"
     ]
    }
   ],
   "source": [
    "y_data = y\n",
    "y_predict_KNN = KNN_classifier.predict(x)\n",
    "\n",
    "print(y_predict_KNN.shape)\n",
    "print(x.shape)\n",
    "\n",
    "sse =(y_data - y_predict_KNN)**2\n",
    "print (sse)\n",
    "mse = numpy.mean(sse)\n",
    "print (mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_SVM = svm_classifier.predict(x)\n",
    "y_predict_KNN = KNN_classifier.predict(x)\n",
    "y_predict_RF = rf.predict(x)\n",
    "y_predict_m_nb =m_nb.predict(x)\n",
    "y_predict_lr = lr.predict(x)\n",
    "y_predict_dtc = dtc.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(y_data, y_predict_KNN )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_sqr_err_svm = (y_data - y_predict_SVM)**2\n",
    "sum_sqr_err_knn = (y_data - y_predict_KNN)**2\n",
    "sum_sqr_err_rf = (y_data - y_predict_RF)**2\n",
    "sum_sqr_err_nb = (y_data - y_predict_m_nb)**2\n",
    "sum_sqr_err_lr = (y_data - y_predict_lr)**2\n",
    "sum_sqr_err_dtc = (y_data - y_predict_dtc)**2\n",
    "\n",
    "\n",
    "mse_svm = mean_squared_error(y_data, y_predict_SVM )\n",
    "mse_knn = mean_squared_error(y_data, y_predict_KNN )\n",
    "mse_rf = mean_squared_error(y_data, y_predict_RF )\n",
    "mse_nb = mean_squared_error(y_data, y_predict_m_nb )\n",
    "mse_lr = mean_squared_error(y_data, y_predict_lr )\n",
    "mse_dtc = mean_squared_error(y_data, y_predict_dtc )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>Classification Algorithm            </th><th style=\"text-align: right;\">  Squared Error</th><th style=\"text-align: right;\">  Mean Square Error(MSE)</th><th style=\"text-align: right;\">  Root Mean Square Error(MSE)</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>Support Verctor Machine (SVM)       </td><td style=\"text-align: right;\">            527</td><td style=\"text-align: right;\">                0.289243</td><td style=\"text-align: right;\">                     0.537813</td></tr>\n",
       "<tr><td>K-Nearest Neighbour Classifier (KNN)</td><td style=\"text-align: right;\">            886</td><td style=\"text-align: right;\">                0.486279</td><td style=\"text-align: right;\">                     0.697337</td></tr>\n",
       "<tr><td>Random Forest Classifier (RF)       </td><td style=\"text-align: right;\">             46</td><td style=\"text-align: right;\">                0.025247</td><td style=\"text-align: right;\">                     0.158893</td></tr>\n",
       "<tr><td>Naive Bayes Classifier (NB)         </td><td style=\"text-align: right;\">            744</td><td style=\"text-align: right;\">                0.408342</td><td style=\"text-align: right;\">                     0.639017</td></tr>\n",
       "<tr><td>Logistic Regression Classifier (LR) </td><td style=\"text-align: right;\">            552</td><td style=\"text-align: right;\">                0.302964</td><td style=\"text-align: right;\">                     0.550421</td></tr>\n",
       "<tr><td>Decision Tree Classifier (DTC)      </td><td style=\"text-align: right;\">            491</td><td style=\"text-align: right;\">                0.269484</td><td style=\"text-align: right;\">                     0.519119</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "import math\n",
    "table = [[\"Classification Algorithm\",    \"Squared Error\", \"Mean Square Error(MSE)\", \"Root Mean Square Error(MSE)\"],\n",
    "         [\"Support Verctor Machine (SVM)\",         sum(sse1), mse_svm, math.sqrt(mse_svm)],\n",
    "         [\"K-Nearest Neighbour Classifier (KNN)\",  sum(sse2), mse_knn, math.sqrt(mse_knn)],\n",
    "         [\"Random Forest Classifier (RF)\",         sum(sse3), mse_rf, math.sqrt(mse_rf)],\n",
    "         [\"Naive Bayes Classifier (NB)  \",         sum(sse4), mse_nb, math.sqrt(mse_nb)],\n",
    "         [\"Logistic Regression Classifier (LR)\",   sum(sse5), mse_lr, math.sqrt(mse_lr)],\n",
    "         [\"Decision Tree Classifier (DTC) \",       sum(sse6), mse_dtc, math.sqrt(mse_dtc)]]\n",
    "display(HTML(tabulate.tabulate(table, headers='firstrow',tablefmt='html')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
