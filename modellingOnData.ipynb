{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "charged-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries and packages\n",
    "import pandas as pandas\n",
    "import numpy as numpy\n",
    "import matplotlib.pyplot\n",
    "from IPython.display import Markdown, display\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "authorized-denmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data from csv file and Separating table into values(x) and outcome(y)\n",
    "\n",
    "data_table = pandas.read_csv('/Users/banks007/Desktop/Archive/historical_table_adjusted_2.csv', sep=',', header=0)\n",
    "x = data_table.values[:,0:9]\n",
    "y = data_table.values[:,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "complimentary-central",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into training dataset and testing dataset: train (1457 rows), test (365 rows)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=10, test_size = 0.2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "collective-opening",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7178082191780822"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying Support vector machine (SVM) Classifier to Data\n",
    "\n",
    "from sklearn import svm\n",
    "svm_classifier = svm.SVC(kernel='linear', gamma='auto', C=2)\n",
    "svm_classifier.fit(x_train, y_train)\n",
    "svm_classifier.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "governing-strain",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9397260273972603"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying Random Forest Classifier to Data\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=40)\n",
    "rf.fit(x_train, y_train)\n",
    "rf.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "cardiac-imagination",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5589041095890411"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying Naive Bayes Multinomial Classifier to Data\n",
    "\n",
    "m_nb = MultinomialNB()\n",
    "m_nb.fit(x_train, y_train)\n",
    "m_nb.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "liable-anniversary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6986301369863014"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying  Logistic Regression Classifier to Data \n",
    "\n",
    "lr = LogisticRegression(solver = 'liblinear', multi_class='ovr')\n",
    "lr.fit(x_train, y_train)\n",
    "lr.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "overall-canon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7315068493150685"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying Decision Tree Classifier to data\n",
    "    \n",
    "dtc = DecisionTreeClassifier(criterion = \"entropy\", random_state=100, max_depth=3, min_samples_leaf=5)\n",
    "dtc.fit(x_train, y_train)\n",
    "dtc.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "standing-victor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9450926561427591"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying K-Nearest Neighbor (KNN) Classifier to data\n",
    "# preprocessing values for KNN\n",
    "sc_x = StandardScaler()\n",
    "x_train = sc_x.fit_transform(x_train)\n",
    "x_test= sc_x.transform(x_test)\n",
    "\n",
    "KNN_classifier = KNeighborsClassifier(n_neighbors=2, p=2, metric='euclidean')\n",
    "KNN_classifier.fit(x_train, y_train)\n",
    "KNN_classifier.score(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italian-layer",
   "metadata": {},
   "source": [
    "### Implementing K - Fold on Data\n",
    "\n",
    "K-Folds cross-validator\n",
    "\n",
    "Provides train/test indices to split data in train/test sets. Split\n",
    "dataset into k consecutive folds (without shuffling by default).\n",
    "\n",
    "Each fold is then used once as a validation while the k - 1 remaining\n",
    "folds form the training set. \n",
    "\n",
    "K-fold allows all the data to be used in training and testing of the data in other to allow the model to maximise learning and testing with all available data. This helps the model to learn better and give more accurate classification and prediction.\n",
    "\n",
    "K- fold does a specified round of learning on the Data and in each round it changes the part of the data used in testing and training to maximise the learning of the patterns in the table and maximise prediction result\n",
    "table is split into 20 parts and the correspoing parts are used for trainnig and testing <br>\n",
    "\n",
    "|Round|  Testing|Training|\n",
    "|-----|-------|------|\n",
    "|1    | 5  6  7  8  9 10 11 12 13 14 15 16 17 18 19     |  0 1 2 3 4     |\n",
    "|2    | 0  1  2  3  4 10 11 12 13 14 15 16 17 18 19      |  5 6 7 8 9      |\n",
    "|3    | 0  1  2  3  4  5  6  7  8  9 15 16 17 18 19     | 10 11 12 13 14     |\n",
    "|4    | 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14      | 15 16 17 18 19     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "noble-expression",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KFold(n_splits=4, random_state=None, shuffle=False)"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# implementing K-fold on dataset\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "k_fold = KFold(n_splits=4)\n",
    "k_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "paperback-injection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5  6  7  8  9 10 11 12 13 14 15 16 17 18 19] [0 1 2 3 4]\n",
      "[ 0  1  2  3  4 10 11 12 13 14 15 16 17 18 19] [5 6 7 8 9]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 15 16 17 18 19] [10 11 12 13 14]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14] [15 16 17 18 19]\n"
     ]
    }
   ],
   "source": [
    "# creating indices for k-fold\n",
    "\n",
    "for train_index, test_index in k_fold.split([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]):\n",
    "    print(train_index, test_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming-tunisia",
   "metadata": {},
   "source": [
    "### Creating a function to run supplied classifier/algorithm and return score\n",
    "The function take classifier/model type and run it against the test and train data to get a score for the specific model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "false-dynamics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function that takes specific model type and data, use data to learn and test to test model and return score\n",
    "\n",
    "def get_score(model, x_train, x_test, y_train, y_test):\n",
    "    model.fit(x_train, y_train)\n",
    "    \n",
    "    return model.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "described-reality",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a sparate fucntion for KNN\n",
    "\n",
    "def get_score_KNN(model, x_train, x_test, y_train, y_test):\n",
    "#    sc_x = StandardScaler()\n",
    "    x_train = sc_x.fit_transform(x_train)\n",
    "    x_test = sc_x.transform(x_test)\n",
    "\n",
    "    #KNN_classifier = KNeighborsClassifier()\n",
    "    model.fit(x_train, y_train)\n",
    "#    KNN_classifier.score(x_train, y_train)\n",
    "    return model.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "linear-laser",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing array to store each models' score from the k-fold iterations\n",
    "scores_svm = []\n",
    "scores_KNN = []\n",
    "scores_rf = []\n",
    "scores_lr = []\n",
    "scores_n_bayes = []\n",
    "scores_dtc = []\n",
    "scores_knnc = []\n",
    "\n",
    "for train_index, test_index in k_fold.split(x):\n",
    "    x_train, x_test, y_train, y_test = x[train_index], x[test_index], y[train_index], y[test_index]\n",
    "    \n",
    "    scores_svm.append(get_score(SVC(), x_train, x_test, y_train, y_test))\n",
    "    scores_KNN.append(get_score_KNN(KNeighborsClassifier(n_neighbors=2, p=2, metric='euclidean'), x_train, x_test, y_train, y_test))\n",
    "    scores_rf.append(get_score(RandomForestClassifier(), x_train, x_test, y_train, y_test))\n",
    "    scores_lr.append(get_score(LogisticRegression(solver = 'liblinear', multi_class='ovr'), x_train, x_test, y_train, y_test))    \n",
    "    scores_n_bayes.append(get_score(MultinomialNB(), x_train, x_test, y_train, y_test))\n",
    "    scores_dtc.append(get_score(DecisionTreeClassifier(), x_train, x_test, y_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "shared-uganda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8961228968544257"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scores_KNN\n",
    "x_train = sc_x.fit_transform(x_train)\n",
    "x_test = sc_x.transform(x_test)\n",
    "KNN_classifier.score(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "assigned-density",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7472527472527473"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_score_KNN(KNeighborsClassifier(n_neighbors=2, p=2, metric='euclidean'), x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "raising-complex",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM: [0.4342105263157895, 0.5855263157894737, 0.589010989010989, 0.6021978021978022] Avg = 0.5527364083285137\n",
      "NBA: [0.4342105263157895, 0.5855263157894737, 0.6527472527472528, 0.6945054945054945] Avg = 0.5917473973395027\n",
      "LRA: [0.6359649122807017, 0.7039473684210527, 0.6263736263736264, 0.621978021978022] Avg = 0.6470659822633507\n",
      "RFT: [0.8399122807017544, 0.8925438596491229, 0.9494505494505494, 0.8087912087912088] Avg = 0.8726744746481587\n",
      "DTC: [0.8289473684210527, 0.881578947368421, 0.9384615384615385, 0.7868131868131868] Avg = 0.8589502602660497\n",
      "KNN: [0.7653508771929824, 0.7982456140350878, 0.8813186813186813, 0.7472527472527473] Avg = 0.7980419799498747\n"
     ]
    }
   ],
   "source": [
    "avg_svm = sum(scores_svm)/ len(scores_svm)\n",
    "avg_n_bayes  = sum(scores_n_bayes)/ len(scores_n_bayes)\n",
    "avg_lr = sum(scores_lr)/ len(scores_lr)\n",
    "avg_KNN = sum(scores_KNN)/ len(scores_KNN)\n",
    "avg_r_forest = sum(scores_rf)/ len(scores_rf)\n",
    "avg_dtc = sum(scores_dtc)/ len(scores_dtc)\n",
    "\n",
    "print ('SVM: {0} Avg = {1}'.format(scores_svm, avg_svm))\n",
    "print ('NBA: {0} Avg = {1}'.format(scores_n_bayes, avg_n_bayes))\n",
    "print ('LRA: {0} Avg = {1}'.format(scores_lr, avg_lr))\n",
    "print ('RFT: {0} Avg = {1}'.format(scores_rf, avg_r_forest))\n",
    "print ('DTC: {0} Avg = {1}'.format(scores_dtc, avg_dtc))\n",
    "print ('KNN: {0} Avg = {1}'.format(scores_KNN, avg_KNN))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saved-effort",
   "metadata": {},
   "source": [
    "### Printing Result of various classifier on Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "preliminary-earthquake",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Support Vector Machine Classifier**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.55      1.00      0.71       248\n",
      "         1.0       0.00      0.00      0.00       207\n",
      "\n",
      "    accuracy                           0.55       455\n",
      "   macro avg       0.27      0.50      0.35       455\n",
      "weighted avg       0.30      0.55      0.38       455\n",
      "\n",
      "Accuracy is =  54.505494505494504\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/banks007/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Naive Bayes - Multinomial Classifier**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.76      0.35      0.48       248\n",
      "         1.0       0.53      0.87      0.66       207\n",
      "\n",
      "    accuracy                           0.59       455\n",
      "   macro avg       0.65      0.61      0.57       455\n",
      "weighted avg       0.66      0.59      0.56       455\n",
      "\n",
      "Accuracy is =  58.681318681318686\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Random Forest Tree Classifier**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.05      0.09       248\n",
      "         1.0       0.46      0.99      0.63       207\n",
      "\n",
      "    accuracy                           0.48       455\n",
      "   macro avg       0.66      0.52      0.36       455\n",
      "weighted avg       0.68      0.48      0.34       455\n",
      "\n",
      "Accuracy is =  47.69230769230769\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**K-Nearest Neighbor Classifier**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.98      0.92       248\n",
      "         1.0       0.98      0.82      0.89       207\n",
      "\n",
      "    accuracy                           0.91       455\n",
      "   macro avg       0.92      0.90      0.91       455\n",
      "weighted avg       0.92      0.91      0.91       455\n",
      "\n",
      "Accuracy is =  90.98901098901099\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Logistic Regression Classifier**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.55      1.00      0.71       248\n",
      "         1.0       1.00      0.01      0.02       207\n",
      "\n",
      "    accuracy                           0.55       455\n",
      "   macro avg       0.77      0.50      0.36       455\n",
      "weighted avg       0.75      0.55      0.39       455\n",
      "\n",
      "Accuracy is =  54.94505494505495\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Decision Tree Classifier**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.55      1.00      0.71       248\n",
      "         1.0       0.00      0.00      0.00       207\n",
      "\n",
      "    accuracy                           0.55       455\n",
      "   macro avg       0.27      0.50      0.35       455\n",
      "weighted avg       0.30      0.55      0.38       455\n",
      "\n",
      "Accuracy is =  54.505494505494504\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_predict_svm = svm_classifier.predict(x_test)\n",
    "y_predict_KNN = KNN_classifier.predict(x_test)\n",
    "y_predict_rf = rf.predict(x_test)\n",
    "y_predict_m_nb = m_nb.predict(x_test)\n",
    "y_predict_lr = lr.predict(x_test)\n",
    "y_predict_dtc = dtc.predict(x_test)\n",
    "\n",
    "\n",
    "print('')\n",
    "display(Markdown('**Support Vector Machine Classifier**'))\n",
    "print(classification_report(y_test, y_predict_svm))\n",
    "print(\"Accuracy is = \", accuracy_score(y_test, y_predict_svm)*100)\n",
    "print('')\n",
    "print('')\n",
    "\n",
    "display(Markdown('**Naive Bayes - Multinomial Classifier**'))\n",
    "print(classification_report(y_test, y_predict_m_nb))\n",
    "print(\"Accuracy is = \", accuracy_score(y_test, y_predict_m_nb)*100)\n",
    "print('')\n",
    "print('')\n",
    "\n",
    "display(Markdown('**Random Forest Tree Classifier**'))\n",
    "print(classification_report(y_test, y_predict_rf))\n",
    "print(\"Accuracy is = \", accuracy_score(y_test, y_predict_rf)*100)\n",
    "print('')\n",
    "print('')\n",
    "\n",
    "print('')\n",
    "display(Markdown('**K-Nearest Neighbor Classifier**'))\n",
    "print(classification_report(y_test, y_predict_KNN))\n",
    "print(\"Accuracy is = \", accuracy_score(y_test, y_predict_KNN)*100)\n",
    "print('')\n",
    "print('')\n",
    "\n",
    "display(Markdown('**Logistic Regression Classifier**'))\n",
    "print(classification_report(y_test, y_predict_lr))\n",
    "print(\"Accuracy is = \", accuracy_score(y_test, y_predict_lr)*100)\n",
    "print('')\n",
    "print('')\n",
    "\n",
    "display(Markdown('**Decision Tree Classifier**'))\n",
    "print(classification_report(y_test, y_predict_dtc))\n",
    "print(\"Accuracy is = \", accuracy_score(y_test, y_predict_dtc)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "electric-drive",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7186813186813187"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_score(KNeighborsClassifier(), x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "covered-london",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
